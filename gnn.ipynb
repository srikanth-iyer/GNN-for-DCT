{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available()\n",
    "print(torch.cuda.device_count())\n",
    "# print(torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/BU/siyer5/Documents/GNN-for-DCT/.venv/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Generate a 1D Lattice Graph with 3 neighbors on each side\n",
    "def create_1d_lattice_graph(lattice_size=149, neighbors=3):\n",
    "    edges = []\n",
    "    for i in range(lattice_size):\n",
    "        edges.append([i, i])  # self loop\n",
    "        for j in range(1, neighbors + 1):\n",
    "            edges.append([i, (i - j) % lattice_size])  # Connect to left neighbors\n",
    "            edges.append([i, (i + j) % lattice_size])  # Connect to right neighbors\n",
    "\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    return edge_index\n",
    "\n",
    "# Step 2: Define the GNN Model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First GCN layer + ReLU activation\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        # Second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "def generate_data(lattice_size=149, sample_size=100000, prob=0.5):\n",
    "    if prob < 0 or prob > 1:\n",
    "        raise ValueError('Probability must be between 0 and 1.')\n",
    "    \n",
    "    edge_index = create_1d_lattice_graph(lattice_size=lattice_size)\n",
    "    data_list = []\n",
    "    \n",
    "    for _ in range(sample_size):\n",
    "        if prob == 0.5:\n",
    "            # Generate random input states for each node\n",
    "            x = torch.randint(0, 2, (lattice_size, 1), dtype=torch.float)\n",
    "        elif prob == 0.0:\n",
    "            # Generate all zeros\n",
    "            x = torch.zeros(lattice_size, 1)\n",
    "        elif prob == 1.0:\n",
    "            # Generate all ones\n",
    "            x = torch.ones(lattice_size, 1)\n",
    "        else:\n",
    "            x = torch.bernoulli(prob * torch.ones(lattice_size, 1))\n",
    "        \n",
    "        # Define the binary density classification based on the mean of cell states\n",
    "        y = (x.mean() > 0.5).long().repeat(lattice_size)  # 0 for < 0.5 density, 1 for >= 0.5, repeated to match lattice size\n",
    "        \n",
    "        # Create a Data object and append to the list\n",
    "        data_list.append(Data(x=x, edge_index=edge_index, y=y))\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "# Step 3: Create the data and model\n",
    "lattice_size = 149\n",
    "prob = 0.49\n",
    "train_data_list = generate_data(lattice_size=lattice_size, sample_size=20000, prob=prob)\n",
    "test_data_list = generate_data(lattice_size=lattice_size, sample_size=20000, prob=prob)\n",
    "\n",
    "train_loader = DataLoader(train_data_list, batch_size=4098, shuffle=True)\n",
    "test_loader = DataLoader(test_data_list, batch_size=4098, shuffle=False)\n",
    "\n",
    "# Define model, optimizer, and loss function\n",
    "model = GCN(input_dim=1, hidden_dim=32, output_dim=2)  # 2 for binary classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.014)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Step 4: Training Loop\n",
    "def train(loader, model, optimizer, criterion, epochs=400, patience=40):\n",
    "    model.train()\n",
    "    recent_losses = []\n",
    "    avg_loss_last=[]\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for data in loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index)  # Forward pass\n",
    "            loss = criterion(out, data.y)         # Compute the loss\n",
    "            loss.backward()                       # Backpropagation\n",
    "            optimizer.step()                      # Update the parameters\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(loader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss}')\n",
    "        \n",
    "        # Early stopping if average loss over the last `patience` epochs doesn't reduce\n",
    "        recent_losses.append(avg_loss)\n",
    "        if len(recent_losses) > patience:\n",
    "            avg_loss_last.append(sum(recent_losses) / len(recent_losses))\n",
    "            recent_losses.pop(0)\n",
    "            if len(avg_loss_last) > 1 and avg_loss_last[-2] < avg_loss_last[-1]:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "            \n",
    "\n",
    "# Step 5: Testing Loop\n",
    "def test(loader, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            out = model(data.x, data.edge_index)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == data.y).sum().item()\n",
    "    accuracy = correct / (len(loader.dataset) * lattice_size)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Train the model\n",
    "# train(train_loader, model, optimizer, criterion, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# probabilities = [i * 0.01 for i in range(101)]\n",
    "# accuracies = []\n",
    "\n",
    "# for prob in probabilities:\n",
    "#     sample_test_data = generate_data(lattice_size=lattice_size, sample_size=10000, prob=prob)\n",
    "#     sample_test_loader = DataLoader(sample_test_data, batch_size=4098, shuffle=False)\n",
    "#     correct = 0\n",
    "#     model.eval()\n",
    "#     for data in sample_test_loader:\n",
    "#         out = model(data.x, data.edge_index)\n",
    "#         pred = out.argmax(dim=1)\n",
    "#         correct += (pred == data.y).sum().item()\n",
    "#     accuracy = correct / (len(sample_test_loader.dataset) * lattice_size)\n",
    "#     accuracies.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35000000000000003, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41000000000000003, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47000000000000003, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.5700000000000001, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.6900000000000001, 0.7000000000000001, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.8200000000000001, 0.8300000000000001, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.9400000000000001, 0.9500000000000001, 0.96, 0.97, 0.98, 0.99, 1.0]\n",
      "[0.   0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13\n",
      " 0.14 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27\n",
      " 0.28 0.29 0.3  0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41\n",
      " 0.42 0.43 0.44 0.45 0.46 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55\n",
      " 0.56 0.57 0.58 0.59 0.6  0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69\n",
      " 0.7  0.71 0.72 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.8  0.81 0.82 0.83\n",
      " 0.84 0.85 0.86 0.87 0.88 0.89 0.9  0.91 0.92 0.93 0.94 0.95 0.96 0.97\n",
      " 0.98 0.99 1.  ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# plt.plot(probabilities, accuracies)\n",
    "# plt.xlabel('Probability')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Accuracy vs Probability')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400, Loss: 0.6657489180564881\n",
      "Epoch 2/400, Loss: 0.6003825545310975\n",
      "Epoch 3/400, Loss: 0.5404214859008789\n",
      "Epoch 4/400, Loss: 0.4860027253627777\n",
      "Epoch 5/400, Loss: 0.43708619475364685\n",
      "Epoch 6/400, Loss: 0.3934718072414398\n",
      "Epoch 7/400, Loss: 0.35483341813087466\n",
      "Epoch 8/400, Loss: 0.32076109051704405\n",
      "Epoch 9/400, Loss: 0.2908013999462128\n",
      "Epoch 10/400, Loss: 0.26449201703071595\n",
      "Epoch 11/400, Loss: 0.2413867622613907\n",
      "Epoch 12/400, Loss: 0.22107152342796327\n",
      "Epoch 13/400, Loss: 0.2031730592250824\n",
      "Epoch 14/400, Loss: 0.18736161291599274\n",
      "Epoch 15/400, Loss: 0.17335015237331391\n",
      "Epoch 16/400, Loss: 0.1608915477991104\n",
      "Epoch 17/400, Loss: 0.14977436661720275\n",
      "Epoch 18/400, Loss: 0.13981837928295135\n",
      "Epoch 19/400, Loss: 0.13087020218372344\n",
      "Epoch 20/400, Loss: 0.122799614071846\n",
      "Epoch 21/400, Loss: 0.11549558341503144\n",
      "Epoch 22/400, Loss: 0.10886370837688446\n",
      "Epoch 23/400, Loss: 0.10282309800386429\n",
      "Epoch 24/400, Loss: 0.09730463176965713\n",
      "Epoch 25/400, Loss: 0.09224874526262283\n",
      "Epoch 26/400, Loss: 0.08760414868593216\n",
      "Epoch 27/400, Loss: 0.0833264097571373\n",
      "Epoch 28/400, Loss: 0.07937691509723663\n",
      "Epoch 29/400, Loss: 0.07572205066680908\n",
      "Epoch 30/400, Loss: 0.07233252674341202\n",
      "Epoch 31/400, Loss: 0.06918242424726487\n",
      "Epoch 32/400, Loss: 0.06624906212091446\n",
      "Epoch 33/400, Loss: 0.06351245567202568\n",
      "Epoch 34/400, Loss: 0.06095480918884277\n",
      "Epoch 35/400, Loss: 0.058560362458229064\n",
      "Epoch 36/400, Loss: 0.05631502866744995\n",
      "Epoch 37/400, Loss: 0.05420632213354111\n",
      "Epoch 38/400, Loss: 0.05222300142049789\n",
      "Epoch 39/400, Loss: 0.05035494416952133\n",
      "Epoch 40/400, Loss: 0.04859310090541839\n",
      "Epoch 41/400, Loss: 0.046929285675287244\n",
      "Epoch 42/400, Loss: 0.045356068015098575\n",
      "Epoch 43/400, Loss: 0.043866807222366334\n",
      "Epoch 44/400, Loss: 0.0424553856253624\n",
      "Epoch 45/400, Loss: 0.041116277873516086\n",
      "Epoch 46/400, Loss: 0.03984447866678238\n",
      "Epoch 47/400, Loss: 0.038635363429784776\n",
      "Epoch 48/400, Loss: 0.03748467043042183\n",
      "Epoch 49/400, Loss: 0.03638859316706657\n",
      "Epoch 50/400, Loss: 0.03534364253282547\n",
      "Epoch 51/400, Loss: 0.034346532076597214\n",
      "Epoch 52/400, Loss: 0.03339428827166557\n",
      "Epoch 53/400, Loss: 0.03248411789536476\n",
      "Epoch 54/400, Loss: 0.03161351084709167\n",
      "Epoch 55/400, Loss: 0.03078012093901634\n",
      "Epoch 56/400, Loss: 0.029981748759746553\n",
      "Epoch 57/400, Loss: 0.02921643704175949\n",
      "Epoch 58/400, Loss: 0.028482236340641974\n",
      "Epoch 59/400, Loss: 0.027777454629540444\n",
      "Epoch 60/400, Loss: 0.027100514993071555\n",
      "Epoch 61/400, Loss: 0.02644990161061287\n",
      "Epoch 62/400, Loss: 0.025824148580431937\n",
      "Epoch 63/400, Loss: 0.025222032517194747\n",
      "Epoch 64/400, Loss: 0.024642239883542062\n",
      "Epoch 65/400, Loss: 0.024083786457777024\n",
      "Epoch 66/400, Loss: 0.023545439913868906\n",
      "Epoch 67/400, Loss: 0.023026282712817193\n",
      "Epoch 68/400, Loss: 0.022525368630886076\n",
      "Epoch 69/400, Loss: 0.022041811421513558\n",
      "Epoch 70/400, Loss: 0.02157479412853718\n",
      "Epoch 71/400, Loss: 0.021123526990413664\n",
      "Epoch 72/400, Loss: 0.020687303319573403\n",
      "Epoch 73/400, Loss: 0.020265399292111397\n",
      "Epoch 74/400, Loss: 0.019857225567102434\n",
      "Epoch 75/400, Loss: 0.019462142884731293\n",
      "Epoch 76/400, Loss: 0.019079533964395524\n",
      "Epoch 77/400, Loss: 0.01870892532169819\n",
      "Epoch 78/400, Loss: 0.018349742516875267\n",
      "Epoch 79/400, Loss: 0.018001531809568407\n",
      "Epoch 80/400, Loss: 0.017663852125406266\n",
      "Epoch 81/400, Loss: 0.017336209118366242\n",
      "Epoch 82/400, Loss: 0.017018232122063638\n",
      "Epoch 83/400, Loss: 0.01670950949192047\n",
      "Epoch 84/400, Loss: 0.01640969663858414\n",
      "Epoch 85/400, Loss: 0.016118453815579414\n",
      "Epoch 86/400, Loss: 0.015835338830947877\n",
      "Epoch 87/400, Loss: 0.015560194663703442\n",
      "Epoch 88/400, Loss: 0.015292603895068169\n",
      "Epoch 89/400, Loss: 0.015032314881682395\n",
      "Epoch 90/400, Loss: 0.01477905809879303\n",
      "Epoch 91/400, Loss: 0.014532548375427723\n",
      "Epoch 92/400, Loss: 0.014292586036026478\n",
      "Epoch 93/400, Loss: 0.014058861881494522\n",
      "Epoch 94/400, Loss: 0.01383121870458126\n",
      "Epoch 95/400, Loss: 0.01360938511788845\n",
      "Epoch 96/400, Loss: 0.013393193110823631\n",
      "Epoch 97/400, Loss: 0.01318248249590397\n",
      "Epoch 98/400, Loss: 0.012976954132318497\n",
      "Epoch 99/400, Loss: 0.012776536121964454\n",
      "Epoch 100/400, Loss: 0.012581005319952964\n",
      "Epoch 101/400, Loss: 0.012390163727104664\n",
      "Epoch 102/400, Loss: 0.012203947268426418\n",
      "Epoch 103/400, Loss: 0.012022131495177746\n",
      "Epoch 104/400, Loss: 0.011844613775610923\n",
      "Epoch 105/400, Loss: 0.011671186052262783\n",
      "Epoch 106/400, Loss: 0.011501812562346458\n",
      "Epoch 107/400, Loss: 0.011336292885243893\n",
      "Epoch 108/400, Loss: 0.011174499802291394\n",
      "Epoch 109/400, Loss: 0.011016403883695602\n",
      "Epoch 110/400, Loss: 0.010861820913851261\n",
      "Epoch 111/400, Loss: 0.010710610821843148\n",
      "Epoch 112/400, Loss: 0.010562796890735627\n",
      "Epoch 113/400, Loss: 0.010418101772665977\n",
      "Epoch 114/400, Loss: 0.010276593454182147\n",
      "Epoch 115/400, Loss: 0.010138087905943394\n",
      "Epoch 116/400, Loss: 0.010002534836530685\n",
      "Epoch 117/400, Loss: 0.009869821555912494\n",
      "Epoch 118/400, Loss: 0.00973985269665718\n",
      "Epoch 119/400, Loss: 0.009612582251429557\n",
      "Epoch 120/400, Loss: 0.009487916901707649\n",
      "Epoch 121/400, Loss: 0.009365789033472538\n",
      "Epoch 122/400, Loss: 0.009246148727834224\n",
      "Epoch 123/400, Loss: 0.009128926694393158\n",
      "Epoch 124/400, Loss: 0.009014006145298481\n",
      "Epoch 125/400, Loss: 0.008901319280266762\n",
      "Epoch 126/400, Loss: 0.008790887705981732\n",
      "Epoch 127/400, Loss: 0.008682594448328019\n",
      "Epoch 128/400, Loss: 0.00857636947184801\n",
      "Epoch 129/400, Loss: 0.008472190424799918\n",
      "Epoch 130/400, Loss: 0.008369963988661766\n",
      "Epoch 131/400, Loss: 0.008269734494388104\n",
      "Epoch 132/400, Loss: 0.008171318285167217\n",
      "Epoch 133/400, Loss: 0.008074759133160114\n",
      "Epoch 134/400, Loss: 0.007979986630380154\n",
      "Epoch 135/400, Loss: 0.007886935397982598\n",
      "Epoch 136/400, Loss: 0.007795553468167782\n",
      "Epoch 137/400, Loss: 0.007705867663025856\n",
      "Epoch 138/400, Loss: 0.007617781963199377\n",
      "Epoch 139/400, Loss: 0.007531273271888494\n",
      "Epoch 140/400, Loss: 0.007446272112429142\n",
      "Epoch 141/400, Loss: 0.007362779881805182\n",
      "Epoch 142/400, Loss: 0.007280769664794207\n",
      "Epoch 143/400, Loss: 0.007200150564312935\n",
      "Epoch 144/400, Loss: 0.00712092099711299\n",
      "Epoch 145/400, Loss: 0.00704300943762064\n",
      "Epoch 146/400, Loss: 0.006966491881757975\n",
      "Epoch 147/400, Loss: 0.0068912219256162645\n",
      "Epoch 148/400, Loss: 0.006817248370498419\n",
      "Epoch 149/400, Loss: 0.006744501274079084\n",
      "Epoch 150/400, Loss: 0.006672979984432459\n",
      "Epoch 151/400, Loss: 0.006602567527443171\n",
      "Epoch 152/400, Loss: 0.0065333802253007885\n",
      "Epoch 153/400, Loss: 0.0064652801491320135\n",
      "Epoch 154/400, Loss: 0.006398263480514288\n",
      "Epoch 155/400, Loss: 0.0063323559239506725\n",
      "Epoch 156/400, Loss: 0.006267488095909357\n",
      "Epoch 157/400, Loss: 0.0062036319635808464\n",
      "Epoch 158/400, Loss: 0.006140793673694134\n",
      "Epoch 159/400, Loss: 0.006078992877155543\n",
      "Epoch 160/400, Loss: 0.006018065009266138\n",
      "Epoch 161/400, Loss: 0.0059581280685961245\n",
      "Epoch 162/400, Loss: 0.005899113416671753\n",
      "Epoch 163/400, Loss: 0.005840995814651251\n",
      "Epoch 164/400, Loss: 0.005783751513808966\n",
      "Epoch 165/400, Loss: 0.005727380607277155\n",
      "Epoch 166/400, Loss: 0.00567186065018177\n",
      "Epoch 167/400, Loss: 0.005617167055606842\n",
      "Epoch 168/400, Loss: 0.00556327598169446\n",
      "Epoch 169/400, Loss: 0.0055102122947573665\n",
      "Epoch 170/400, Loss: 0.005457905307412148\n",
      "Epoch 171/400, Loss: 0.0054063541814684864\n",
      "Epoch 172/400, Loss: 0.005355558544397354\n",
      "Epoch 173/400, Loss: 0.005305520631372929\n",
      "Epoch 174/400, Loss: 0.005256190150976181\n",
      "Epoch 175/400, Loss: 0.005207521840929985\n",
      "Epoch 176/400, Loss: 0.005159610509872436\n",
      "Epoch 177/400, Loss: 0.0051123368553817276\n",
      "Epoch 178/400, Loss: 0.005065725278109312\n",
      "Epoch 179/400, Loss: 0.005019800085574388\n",
      "Epoch 180/400, Loss: 0.004974488355219364\n",
      "Epoch 181/400, Loss: 0.004929792415350675\n",
      "Epoch 182/400, Loss: 0.0048857351765036585\n",
      "Epoch 183/400, Loss: 0.004842244926840067\n",
      "Epoch 184/400, Loss: 0.004799417313188314\n",
      "Epoch 185/400, Loss: 0.0047570854425430294\n",
      "Epoch 186/400, Loss: 0.004715393949300051\n",
      "Epoch 187/400, Loss: 0.004674222134053707\n",
      "Epoch 188/400, Loss: 0.004633571021258831\n",
      "Epoch 189/400, Loss: 0.0045935348607599735\n",
      "Epoch 190/400, Loss: 0.004553948063403368\n",
      "Epoch 191/400, Loss: 0.004514929093420506\n",
      "Epoch 192/400, Loss: 0.004476430919021368\n",
      "Epoch 193/400, Loss: 0.00443840641528368\n",
      "Epoch 194/400, Loss: 0.004400855116546154\n",
      "Epoch 195/400, Loss: 0.004363824333995581\n",
      "Epoch 196/400, Loss: 0.004327267501503229\n"
     ]
    }
   ],
   "source": [
    "# training_probabilities = [0.4, 0.41 , 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6]\n",
    "training_probabilities = np.linspace(0, 1, 101)\n",
    "accuracies = {}\n",
    "probabilities = np.linspace(0, 1, 101)\n",
    "\n",
    "for prob in training_probabilities:\n",
    "    train_data_list = generate_data(lattice_size=lattice_size, sample_size=20000, prob=prob)\n",
    "    train_loader = DataLoader(train_data_list, batch_size=4098, shuffle=True)\n",
    "    model = GCN(input_dim=1, hidden_dim=32, output_dim=2)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.014)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    train(train_loader, model, optimizer, criterion, epochs=400)\n",
    "    for probability in probabilities:\n",
    "        sample_test_data = generate_data(lattice_size=lattice_size, sample_size=10000, prob=probability)\n",
    "        sample_test_loader = DataLoader(sample_test_data, batch_size=4098, shuffle=False)\n",
    "        correct = 0\n",
    "        model.eval()\n",
    "        for data in sample_test_loader:\n",
    "            out = model(data.x, data.edge_index)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == data.y).sum().item()\n",
    "        accuracy = correct / (len(sample_test_loader.dataset) * lattice_size)\n",
    "        accuracies[prob] = accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for prob in training_probabilities:\n",
    "    plt.plot(probabilities, [accuracies[prob] for prob in probabilities], label=f'p={prob}')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Probability')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = generate_data(lattice_size=149, sample_size=1)\n",
    "# sample[0].y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
